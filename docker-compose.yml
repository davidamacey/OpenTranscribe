version: '3.8'

services:
  postgres:
    image: postgres:14-alpine
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=opentranscribe
    ports:
      - "5176:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio
    restart: always
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "5178:9000"
      - "5179:9001"
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 10s
      retries: 5

  redis:
    image: redis:7-alpine
    restart: always
    ports:
      - "5177:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50

  opensearch:
    image: opensearchproject/opensearch:2.5.0
    restart: always
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - DISABLE_SECURITY_PLUGIN=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    ports:
      - "5180:9200"
      - "5181:9600"
    healthcheck:
      test: ["CMD-SHELL", "curl -sS http://localhost:9200 || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 20

  # Development services that mirror production setup

  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile.dev
    restart: always
    volumes:
      - ./backend:/app
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/root/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/root/.cache/torch
    ports:
      - "5174:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=opentranscribe
      - MINIO_HOST=minio
      - MINIO_PORT=9000
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OPENSEARCH_HOST=opensearch
      - OPENSEARCH_PORT=9200
      - JWT_SECRET_KEY=dev_secret_key_change_in_production
      - JWT_ALGORITHM=HS256
      - JWT_ACCESS_TOKEN_EXPIRE_MINUTES=60
      - MODELS_DIRECTORY=/app/models
      - MODEL_BASE_DIR=/app/models
      - TEMP_DIR=/app/temp
      - USE_GPU=true
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      - WHISPER_MODEL=large-v2
      - BATCH_SIZE=16
      - COMPUTE_TYPE=float16
      - DIARIZATION_MODEL=pyannote/speaker-diarization-3.1
      - MIN_SPEAKERS=1
      - MAX_SPEAKERS=10
      # LLM Configuration - defaults to external/cloud providers
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://localhost:8012/v1}
      - VLLM_API_KEY=${VLLM_API_KEY:-}
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-gpt-oss-20b}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4o-mini}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama2:7b-chat}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - ANTHROPIC_MODEL_NAME=${ANTHROPIC_MODEL_NAME:-claude-3-haiku-20240307}
      - ANTHROPIC_BASE_URL=${ANTHROPIC_BASE_URL:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_MODEL_NAME=${OPENROUTER_MODEL_NAME:-anthropic/claude-3-haiku}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
    command: uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload

  celery-worker:
    build: 
      context: ./backend
      dockerfile: Dockerfile.dev
    restart: always
    command: celery -A app.core.celery worker --loglevel=info
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['${GPU_DEVICE_ID:-0}']
    volumes:
      - ./backend:/app
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/root/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/root/.cache/torch
    depends_on:
      - postgres
      - redis
      - minio
      - opensearch
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=opentranscribe
      - MINIO_HOST=minio
      - MINIO_PORT=9000
      - MINIO_ROOT_USER=minioadmin
      - MEDIA_BUCKET_NAME=opentranscribe
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      - WHISPER_MODEL=large-v2
      - BATCH_SIZE=16
      - COMPUTE_TYPE=float16
      - DIARIZATION_MODEL=pyannote/speaker-diarization-3.1
      - MIN_SPEAKERS=1
      - MAX_SPEAKERS=10
      - MINIO_ROOT_PASSWORD=minioadmin
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OPENSEARCH_HOST=opensearch
      - MODELS_DIRECTORY=/app/models
      - OPENSEARCH_PORT=9200
      - MODEL_BASE_DIR=/app/models
      - TEMP_DIR=/app/temp
      - USE_GPU=true
      # LLM Configuration - defaults to external/cloud providers
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://localhost:8012/v1}
      - VLLM_API_KEY=${VLLM_API_KEY:-}
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-gpt-oss-20b}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4o-mini}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama2:7b-chat}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - ANTHROPIC_MODEL_NAME=${ANTHROPIC_MODEL_NAME:-claude-3-haiku-20240307}
      - ANTHROPIC_BASE_URL=${ANTHROPIC_BASE_URL:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_MODEL_NAME=${OPENROUTER_MODEL_NAME:-anthropic/claude-3-haiku}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    restart: always
    volumes:
      - ./frontend:/app
      # Exclude node_modules from host to prevent permission issues
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - NODE_ENV=development
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5173"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      backend:
        condition: service_healthy

  frontend-prod:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    restart: unless-stopped
    ports:
      - "5173:80"
    environment:
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy

  flower:
    build: 
      context: ./backend
      dockerfile: Dockerfile.dev
    restart: always
    command: >
      python -m celery -A app.core.celery flower
      --port=5555
      --url_prefix=flower
      --persistent=True
      --db=/app/flower.db
      --broker=redis://redis:6379/0
    ports:
      - "5175:5555"
    depends_on:
      - redis
      - celery-worker
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      # No authentication required as per user requirements
    volumes:
      - ./backend:/app
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/root/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/root/.cache/torch

volumes:
  postgres_data:
  minio_data:
  redis_data:
  opensearch_data:
