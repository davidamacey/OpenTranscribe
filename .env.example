# OpenTranscribe Configuration
# This file works for both development and production
# Adjust values marked with [PROD] or [DEV] as needed

# Database Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5176  # [PROD] Production port / [DEV] Use 5432 for development
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres  # [PROD] Change this in production!
POSTGRES_DB=opentranscribe

# MinIO Object Storage Configuration  
MINIO_HOST=minio
MINIO_PORT=5178      # [PROD] Production port / [DEV] Use 9000 for development
MINIO_CONSOLE_PORT=5179  # [PROD] Production port / [DEV] Use 9001 for development
MINIO_ROOT_USER=minioadmin     # [PROD] Change this in production!
MINIO_ROOT_PASSWORD=minioadmin # [PROD] Change this in production!
MEDIA_BUCKET_NAME=opentranscribe

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=5177  # [PROD] Production port / [DEV] Use 6379 for development

# OpenSearch Configuration
OPENSEARCH_HOST=opensearch
OPENSEARCH_PORT=5180  # [PROD] Production port / [DEV] Use 9200 for development
OPENSEARCH_ADMIN_PORT=5181  # [PROD] Production only

# JWT Authentication
JWT_SECRET_KEY=change_this_in_production  # [PROD] MUST change this in production!
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=1440  # [PROD] 24hrs / [DEV] Use 60 for development

# Model Storage
MODEL_BASE_DIR=/app/models
TEMP_DIR=/app/temp
MODEL_CACHE_DIR=./models         # Directory to store downloaded AI models

# Hardware Detection (auto-detected by setup script)
TORCH_DEVICE=auto  # Options: auto, cuda, mps, cpu
COMPUTE_TYPE=auto  # Options: auto, float16, float32, int8
USE_GPU=auto       # Will be auto-detected
GPU_DEVICE_ID=0

# AI Models Configuration
WHISPER_MODEL=large-v2  # Options: tiny, base, small, medium, large-v1, large-v2
BATCH_SIZE=auto         # Will be auto-detected based on hardware
DIARIZATION_MODEL=pyannote/speaker-diarization-3.1
MIN_SPEAKERS=1
MAX_SPEAKERS=10

# HuggingFace Token (REQUIRED for speaker diarization)
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_huggingface_token_here

# LLM Configuration for AI Summarization and Speaker Identification
# 
# DEPLOYMENT OPTIONS:
# 1. Cloud-only: Set LLM_PROVIDER=openai and configure API keys below
# 2. Local vLLM: Set LLM_PROVIDER=vllm and run: docker compose -f docker-compose.yml -f docker-compose.vllm.yml up  
# 3. Local Ollama: Set LLM_PROVIDER=ollama, uncomment ollama service, run same command
# 4. No LLM: Leave blank or comment out for transcription-only mode
#
# Supported providers: vllm, openai, ollama, anthropic, openrouter
LLM_PROVIDER=openai

# vLLM Configuration (Default provider)
# For running local vLLM server with OpenAI-compatible API
VLLM_BASE_URL=http://localhost:8012/v1  # Your vLLM server endpoint
VLLM_API_KEY=your_vllm_api_key_here     # Optional, if your vLLM requires authentication
VLLM_MODEL_NAME=gpt-oss-20b             # Model name (e.g., gpt-oss, llama-2-7b-chat)

# OpenAI Configuration (Alternative provider)
# Uncomment and configure if using OpenAI instead of vLLM
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL_NAME=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1  # Optional, for custom endpoints

# Ollama Configuration (Alternative provider)
# Uncomment and configure if using Ollama instead of vLLM
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL_NAME=llama2:7b-chat

# Anthropic Claude Configuration (Alternative provider)
# Uncomment and configure if using Claude instead of vLLM
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL_NAME=claude-3-haiku-20240307
# ANTHROPIC_BASE_URL=https://api.anthropic.com  # Optional

# OpenRouter Configuration (Alternative provider)
# Uncomment and configure if using OpenRouter instead of vLLM
# OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_MODEL_NAME=anthropic/claude-3-haiku
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# External Port Configuration
# [PROD] Production uses sequential ports to avoid conflicts
# [DEV] Development can use standard ports (8080, 5173, etc.)
FRONTEND_PORT=5173   # [DEV] Same for both environments
BACKEND_PORT=5174    # [PROD] Production port / [DEV] Use 8080 for development
FLOWER_PORT=5175     # [PROD] Production port / [DEV] Use 5555 for development  
POSTGRES_PORT=5176   # [PROD] External port / [DEV] Use 5432 for development
REDIS_PORT=5177      # [PROD] External port / [DEV] Use 6379 for development
MINIO_PORT=5178      # [PROD] External port / [DEV] Use 9000 for development
MINIO_CONSOLE_PORT=5179  # [PROD] External port / [DEV] Use 9001 for development
OPENSEARCH_PORT=5180     # [PROD] External port / [DEV] Use 9200 for development
OPENSEARCH_ADMIN_PORT=5181  # [PROD] Production only

# Frontend Configuration
NODE_ENV=production  # [PROD] production / [DEV] development
VITE_FLOWER_URL_PREFIX=flower

# API URLs for Frontend
# [PROD] Production URLs (use with sequential ports):
VITE_API_BASE_URL=http://localhost:5174/api  
VITE_WS_BASE_URL=ws://localhost:5174/ws
VITE_FLOWER_PORT=5175

# [DEV] For development, update these to match your backend ports:
# VITE_API_BASE_URL=http://localhost:8080/api  
# VITE_WS_BASE_URL=ws://localhost:8080/ws
# VITE_FLOWER_PORT=5555