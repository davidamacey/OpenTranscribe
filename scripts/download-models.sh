#!/bin/bash
# Exit on error is removed to allow graceful error handling
# set -e  # DO NOT use - we need to handle partial download failures

# OpenTranscribe Model Downloader
# Downloads all required AI models before application startup
# Usage: ./scripts/download-models.sh [model_cache_dir]

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
MODEL_CACHE_DIR="${1:-./models}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

print_header() {
    echo -e "\n${CYAN}================================================================${NC}"
    echo -e "${CYAN}  $1${NC}"
    echo -e "${CYAN}================================================================${NC}\n"
}

print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

# Calculate directory size
get_dir_size() {
    du -sh "$1" 2>/dev/null | cut -f1 || echo "0"
}

check_models_exist() {
    print_info "Checking for existing models in $MODEL_CACHE_DIR..."

    # Check if models directory exists and has content
    if [ -d "$MODEL_CACHE_DIR/huggingface" ] && [ -d "$MODEL_CACHE_DIR/torch" ]; then
        local hf_size
        local torch_size
        hf_size=$(du -sb "$MODEL_CACHE_DIR/huggingface" 2>/dev/null | cut -f1)
        torch_size=$(du -sb "$MODEL_CACHE_DIR/torch" 2>/dev/null | cut -f1)

        # If both directories have substantial content (>100MB combined), assume models exist
        if [ "$((hf_size + torch_size))" -gt 100000000 ]; then
            local total_size
            total_size=$(get_dir_size "$MODEL_CACHE_DIR")
            print_success "Found existing models ($total_size)"
            echo -e "${YELLOW}Do you want to skip model download and use existing models? (Y/n)${NC}"
            read -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Nn]$ ]]; then
                print_info "Skipping model download - using existing models"
                return 0
            else
                print_info "Re-downloading models as requested"
            fi
        fi
    fi

    return 1
}

check_huggingface_token() {
    print_info "Checking for HuggingFace token..."

    # Check environment variable first
    if [ -n "$HUGGINGFACE_TOKEN" ]; then
        print_success "HuggingFace token found in environment"
        return 0
    fi

    # Check .env file
    if [ -f "$REPO_ROOT/.env" ]; then
        local token
        token=$(grep "^HUGGINGFACE_TOKEN=" "$REPO_ROOT/.env" | cut -d'=' -f2 | tr -d ' ')
        if [ -n "$token" ]; then
            export HUGGINGFACE_TOKEN="$token"
            print_success "HuggingFace token loaded from .env file"
            return 0
        fi
    fi

    print_error "HUGGINGFACE_TOKEN not found!"
    echo ""
    echo -e "${YELLOW}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${YELLOW}  HUGGINGFACE TOKEN REQUIRED FOR SPEAKER DIARIZATION${NC}"
    echo -e "${YELLOW}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo ""
    echo -e "${CYAN}To download PyAnnote speaker diarization models, you need:${NC}"
    echo ""
    echo "1. Create a FREE HuggingFace token:"
    echo "   • Visit: https://huggingface.co/settings/tokens"
    echo "   • Click 'New token'"
    echo "   • Give it a name (e.g., 'OpenTranscribe')"
    echo "   • Select 'Read' permissions"
    echo "   • Copy the token"
    echo ""
    echo -e "${RED}2. Accept BOTH gated model agreements (REQUIRED):${NC}"
    echo -e "   ${YELLOW}• Segmentation Model:${NC}"
    echo "     https://huggingface.co/pyannote/segmentation-3.0"
    echo -e "     ${GREEN}→ Click 'Agree and access repository'${NC}"
    echo ""
    echo -e "   ${YELLOW}• Speaker Diarization Model:${NC}"
    echo "     https://huggingface.co/pyannote/speaker-diarization-3.1"
    echo -e "     ${GREEN}→ Click 'Agree and access repository'${NC}"
    echo ""
    echo "3. Configure your token:"
    echo "   • Export it: export HUGGINGFACE_TOKEN=your_token_here"
    echo "   • Or add to .env file: HUGGINGFACE_TOKEN=your_token_here"
    echo ""
    echo -e "${YELLOW}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo ""
    exit 1
}

download_models_docker() {
    print_header "Downloading AI Models"

    print_info "This will download approximately 2.5GB of AI models:"
    print_info "  • WhisperX transcription models (~1.5GB)"
    print_info "  • PyAnnote speaker diarization models (~500MB)"
    print_info "  • Wav2Vec2 alignment model (~360MB)"
    echo ""
    print_warning "This may take 10-30 minutes depending on your internet speed..."
    echo ""

    # Create model cache directories
    mkdir -p "$MODEL_CACHE_DIR/huggingface"
    mkdir -p "$MODEL_CACHE_DIR/torch"

    print_info "Starting model download using Docker..."
    echo ""

    # Get Whisper model from .env or use default
    local whisper_model="large-v2"
    if [ -f "$REPO_ROOT/.env" ]; then
        local env_model
        env_model=$(grep "^WHISPER_MODEL=" "$REPO_ROOT/.env" | cut -d'=' -f2 | tr -d ' ')
        if [ -n "$env_model" ]; then
            whisper_model="$env_model"
        fi
    fi

    # Determine if GPU is available
    local use_gpu="false"
    local gpu_args=""
    if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
        use_gpu="true"
        # Use specific GPU if GPU_DEVICE_ID is set, otherwise use all GPUs
        if [ -n "$GPU_DEVICE_ID" ]; then
            gpu_args="--gpus device=${GPU_DEVICE_ID}"
            print_info "GPU detected - using GPU ${GPU_DEVICE_ID} for model initialization"
        else
            gpu_args="--gpus all"
            print_info "GPU detected - using GPU for faster model initialization"
        fi
    else
        print_info "No GPU detected - using CPU (this is fine, just slower)"
    fi

    # Run model download in Docker container with progress output
    print_info "Downloading models (progress shown below)..."
    echo ""

    # Run the download with real-time output
    # IMPORTANT: Backend runs as 'appuser' (UID 1000), so mount to /home/appuser/.cache
    # Note: When using --gpus device=X, Docker remaps that GPU to index 0 inside container
    # So we always use CUDA_VISIBLE_DEVICES=0 inside the container, regardless of host GPU index
    # shellcheck disable=SC2086
    docker run --rm \
        $gpu_args \
        -e CUDA_VISIBLE_DEVICES=0 \
        -e HUGGINGFACE_TOKEN="${HUGGINGFACE_TOKEN}" \
        -e WHISPER_MODEL="${whisper_model}" \
        -e USE_GPU="${use_gpu}" \
        -e COMPUTE_TYPE="${COMPUTE_TYPE:-float16}" \
        -e DIARIZATION_MODEL="${DIARIZATION_MODEL:-pyannote/speaker-diarization-3.1}" \
        -v "$(realpath "$MODEL_CACHE_DIR/huggingface"):/home/appuser/.cache/huggingface" \
        -v "$(realpath "$MODEL_CACHE_DIR/torch"):/home/appuser/.cache/torch" \
        -v "$SCRIPT_DIR/download-models.py:/app/download-models.py:ro" \
        davidamacey/opentranscribe-backend:latest \
        python /app/download-models.py

    local docker_exit_code=$?

    echo ""

    # Check if download succeeded
    if [ $docker_exit_code -eq 0 ]; then
        local total_size
        total_size=$(get_dir_size "$MODEL_CACHE_DIR")
        print_success "Models downloaded successfully ($total_size)"

        # Create marker file to indicate successful download
        date -u +"%Y-%m-%dT%H:%M:%SZ" > "$MODEL_CACHE_DIR/.download_complete"

        return 0
    else
        # Download failed - provide detailed error information
        print_error "Model download failed (exit code: $docker_exit_code)"
        echo ""

        # Check if any models were partially downloaded
        local hf_size
        local torch_size
        local partial_size
        hf_size=$(du -sb "$MODEL_CACHE_DIR/huggingface" 2>/dev/null | cut -f1 || echo "0")
        torch_size=$(du -sb "$MODEL_CACHE_DIR/torch" 2>/dev/null | cut -f1 || echo "0")
        partial_size=$(get_dir_size "$MODEL_CACHE_DIR")

        # Check if this is likely a gated model access issue
        # Expected size with all models is ~11GB, partial download ~5-6GB suggests PyAnnote models missing
        local expected_min_size=10000000000  # 10GB in bytes
        local has_pyannote_models=false

        if [ -d "$MODEL_CACHE_DIR/torch/pyannote" ]; then
            # Check if PyAnnote models exist
            if [ -d "$MODEL_CACHE_DIR/torch/pyannote/models--pyannote--segmentation-3.0" ] && \
               [ -d "$MODEL_CACHE_DIR/torch/pyannote/models--pyannote--speaker-diarization-3.1" ]; then
                has_pyannote_models=true
            fi
        fi

        if [ "$((hf_size + torch_size))" -gt 1000000 ]; then
            print_warning "Partial download detected ($partial_size)"
            echo "Some models may have been downloaded successfully."

            if [ "$has_pyannote_models" = false ] && [ "$((hf_size + torch_size))" -lt "$expected_min_size" ]; then
                echo ""
                echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
                echo -e "${RED}⚠️  MISSING PYANNOTE MODELS - LIKELY GATED ACCESS ISSUE!${NC}"
                echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
                echo ""
                echo "PyAnnote speaker diarization models were NOT downloaded."
                echo "This usually means you haven't accepted the model agreements."
                echo ""
            fi

            echo "Remaining models will be downloaded on first application use."
        else
            print_error "No models were downloaded"
        fi

        echo ""
        echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        echo -e "${RED}CRITICAL: PyAnnote Models Required for Transcription Pipeline${NC}"
        echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        echo ""
        echo -e "${YELLOW}⚠️  WITHOUT THESE MODELS, ALL TRANSCRIPTIONS WILL FAIL!${NC}"
        echo ""
        echo "The OpenTranscribe pipeline requires speaker diarization models."
        echo "Without them, the entire transcription process cannot complete."
        echo ""
        echo -e "${CYAN}═══════════════════════════════════════════════════════════════════${NC}"
        echo -e "${CYAN}REQUIRED ACTION: Accept BOTH Gated Model Agreements${NC}"
        echo -e "${CYAN}═══════════════════════════════════════════════════════════════════${NC}"
        echo ""
        echo "You MUST accept BOTH of these model agreements on HuggingFace:"
        echo ""
        echo "  1. Segmentation Model:"
        echo "     https://huggingface.co/pyannote/segmentation-3.0"
        echo -e "     ${GREEN}→ Click 'Agree and access repository'${NC}"
        echo ""
        echo "  2. Speaker Diarization Model:"
        echo "     https://huggingface.co/pyannote/speaker-diarization-3.1"
        echo -e "     ${GREEN}→ Click 'Agree and access repository'${NC}"
        echo ""
        echo -e "${CYAN}After accepting BOTH agreements:${NC}"
        echo "  • Wait 1-2 minutes for permissions to propagate"
        echo "  • Run this script again: bash scripts/download-models.sh models"
        echo ""
        echo -e "${YELLOW}Other possible causes (less common):${NC}"
        echo "  • Network connectivity issues"
        echo "  • Invalid HuggingFace token (verify 'Read' permissions)"
        echo "  • Docker image not available"
        echo ""
        echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        echo ""

        return 1
    fi
}

show_summary() {
    print_header "Model Download Summary"

    local total_size
    local hf_size
    local torch_size
    total_size=$(get_dir_size "$MODEL_CACHE_DIR")
    hf_size=$(get_dir_size "$MODEL_CACHE_DIR/huggingface")
    torch_size=$(get_dir_size "$MODEL_CACHE_DIR/torch")

    echo -e "${GREEN}✅ Model cache ready!${NC}"
    echo ""
    echo "Cache location: $MODEL_CACHE_DIR"
    echo "Total size: $total_size"
    echo "  • HuggingFace models: $hf_size"
    echo "  • Torch models: $torch_size"
    echo ""
    print_info "Models are cached and will be available immediately when Docker starts"
    echo ""
}

#######################
# MAIN
#######################

main() {
    print_header "OpenTranscribe Model Downloader"

    print_info "Model cache directory: $MODEL_CACHE_DIR"
    echo ""

    # Check if models already exist
    if check_models_exist; then
        show_summary
        exit 0
    fi

    # Check for HuggingFace token
    check_huggingface_token

    # Download models using Docker (recommended)
    if download_models_docker; then
        show_summary
        exit 0
    else
        print_error "Model download failed"
        echo ""
        print_info "Models will be downloaded automatically when you first run the application,"
        print_info "but this will cause a delay on first use."
        echo ""
        exit 1
    fi
}

# Run main function
main
