# OpenTranscribe Production Overrides
# Use with: docker compose -f docker-compose.yml -f docker-compose.prod.yml up
#
# This file contains ONLY production-specific settings:
#   - Pull pre-built images from Docker Hub
#   - Production volume configurations
#   - Production database initialization path

services:
  backend:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    # No volumes needed - temp files live in container only

  celery-worker:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    volumes:
      # Production: only model cache, no code mounts
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch
      - ${MODEL_CACHE_DIR:-./models}/nltk_data:/home/appuser/.cache/nltk_data
      - ${MODEL_CACHE_DIR:-./models}/sentence-transformers:/home/appuser/.cache/sentence-transformers
      # No temp volume needed - temp files live in container only

  celery-download-worker:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    volumes:
      # Production: only model cache for download worker
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch

  celery-cpu-worker:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    # No volumes needed - CPU worker doesn't need model cache

  celery-nlp-worker:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    # No volumes needed - NLP worker doesn't need model cache

  celery-beat:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    # No volumes needed - scheduler state lives in container only

  frontend:
    image: davidamacey/opentranscribe-frontend:latest
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod  # Production build with NGINX
    pull_policy: always
    ports:
      - "${FRONTEND_PORT:-5173}:8080"  # Production NGINX port
    environment:
      - NODE_ENV=production  # Production mode
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  flower:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always

  # GPU Scaled Worker - Production overrides
  # NOTE: No 'scale' parameter here - base has scale: 0, gpu-scale.yml sets scale: 1
  # This overlay only provides production-specific settings (image, build, pull_policy)
  celery-worker-gpu-scaled:
    image: davidamacey/opentranscribe-backend:latest
    build:
      context: ./backend
      dockerfile: Dockerfile.prod  # Production build
    pull_policy: always
    volumes:
      # Production: Model cache directories
      - ${MODEL_CACHE_DIR:-./models}/huggingface:/home/appuser/.cache/huggingface
      - ${MODEL_CACHE_DIR:-./models}/torch:/home/appuser/.cache/torch
      - ${MODEL_CACHE_DIR:-./models}/nltk_data:/home/appuser/.cache/nltk_data
      - ${MODEL_CACHE_DIR:-./models}/sentence-transformers:/home/appuser/.cache/sentence-transformers
      # No temp volume needed - temp files live in container only
